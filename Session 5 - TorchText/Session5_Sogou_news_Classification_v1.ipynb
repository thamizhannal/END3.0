{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-ov2tcO1pnn"
      },
      "source": [
        "#### Sogou New Dataset Summary\n",
        "The Sogou News dataset is a mixture of 2,909,551 news articles from the SogouCA and SogouCS news corpora, in 5 categories. The number of training samples selected for each class is 90,000 and testing 12,000. \n",
        "\n",
        "**content:** a string feature.\n",
        "**label:** a classification label, with possible values including sports (0), finance (1), entertainment (2), automobile (3), technology (4)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqeGUY8v1oy0"
      },
      "source": [
        "from torchtext.datasets import SogouNews\n",
        "train_iter, test_iter = SogouNews(split=('train', 'test'))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTLsZeef2MDc",
        "outputId": "04fda08d-d51b-493d-c6ba-f307652ee981",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Total number of train data:{}\".format(len(train_iter)))\n",
        "print(\"Total number of test data:{}\".format(len(test_iter)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of train data:450000\n",
            "Total number of test data:60000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbvuK08T8QVA",
        "outputId": "8056bc0b-cfe0-4a81-e61f-d097885039d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# list a sample data.\n",
        "next(train_iter)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4,\n",
              " '2008 di4 qi1 jie4 qi1ng da3o guo2 ji4 che1 zha3n me3i nv3 mo2 te4  2008di4 qi1 jie4 qi1ng da3o guo2 ji4 che1 zha3n yu2 15 ri4 za4i qi1ng da3o guo2 ji4 hui4 zha3n zho1ng xi1n she4ng da4 ka1i mu4 . be3n ci4 che1 zha3n jia1ng chi2 xu4 da4o be3n yue4 19 ri4 . ji1n nia2n qi1ng da3o guo2 ji4 che1 zha3n shi4 li4 nia2n da3o che2ng che1 zha3n gui1 mo2 zui4 da4 di2 yi1 ci4 , shi3 yo4ng lia3o qi1ng da3o guo2 ji4 hui4 zha3n zho1ng xi1n di2 qua2n bu4 shi4 ne4i wa4i zha3n gua3n . yi3 xia4 we2i xia4n cha3ng mo2 te4 tu2 pia4n .')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT3uKvBU2KvE",
        "outputId": "378e8600-bdda-4e5c-93ae-c1976d13ede8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "next(test_iter)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,\n",
              " ' ti3 ca1o shi4 jie4 be1i : che2ng fe1i na2 pi2ng he2ng mu4 zi4 yo2u ca1o ji1n pa2i  su4 du4 : ( shuo1 mi2ng : dia3n ji1 zi4 do4ng bo1 fa4ng )\\\\n  shuo1 mi2ng : dia3n ji1 ga1i a4n niu3 , xua3n ze2 yi1 lu4n ta2n ji2 ke3 ')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV87TZctgvXU"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_iter = SogouNews(split = 'train')\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fPL7nRXgmcQ"
      },
      "source": [
        "dataloader = DataLoader(train_iter, batch_size=64, shuffle=False)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSdi9eJ7YZN0"
      },
      "source": [
        "# initialize tokenizer\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reQIgCaBfhqK"
      },
      "source": [
        "# build basic english tokenizer\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "  for _, text in data_iter:\n",
        "    yield tokenizer(text)\n",
        "\n",
        "# build vocab object.\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siUDwNH0fpv6"
      },
      "source": [
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) - 1"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49fMg0LofrnK"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list.to(device), text_list.to(device), offsets.to(device) "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIj1zjq5fvJC"
      },
      "source": [
        "train_iter = SogouNews(split='train')\n",
        "# data loader using collate batch\n",
        "dataloader = DataLoader(train_iter, batch_size=64, shuffle=False, collate_fn=collate_batch)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebIF9o9Sf6Ob"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxp5t0QJf86g"
      },
      "source": [
        "train_iter = SogouNews(split='train')\n",
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "vocab_size = len(vocab)\n",
        "emsize = 64\n",
        "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S70OsJRjgC4g"
      },
      "source": [
        "import time\n",
        "\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        predited_label = model(text, offsets)\n",
        "        loss = criterion(predited_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1) # disuccees\n",
        "        optimizer.step()\n",
        "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "            predited_label = model(text, offsets)\n",
        "            loss = criterion(predited_label, label)\n",
        "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBD86_SxgDW2",
        "outputId": "27025bde-40de-469a-f876-e575cf77e143"
      },
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "# Hyperparameters\n",
        "EPOCHS = 5 # epoch\n",
        "LR = 5  # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training\n",
        "  \n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "total_accu = None\n",
        "train_iter, test_iter = SogouNews()\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   500/ 6680 batches | accuracy    0.825\n",
            "| epoch   1 |  1000/ 6680 batches | accuracy    0.908\n",
            "| epoch   1 |  1500/ 6680 batches | accuracy    0.917\n",
            "| epoch   1 |  2000/ 6680 batches | accuracy    0.921\n",
            "| epoch   1 |  2500/ 6680 batches | accuracy    0.920\n",
            "| epoch   1 |  3000/ 6680 batches | accuracy    0.925\n",
            "| epoch   1 |  3500/ 6680 batches | accuracy    0.923\n",
            "| epoch   1 |  4000/ 6680 batches | accuracy    0.926\n",
            "| epoch   1 |  4500/ 6680 batches | accuracy    0.926\n",
            "| epoch   1 |  5000/ 6680 batches | accuracy    0.929\n",
            "| epoch   1 |  5500/ 6680 batches | accuracy    0.929\n",
            "| epoch   1 |  6000/ 6680 batches | accuracy    0.928\n",
            "| epoch   1 |  6500/ 6680 batches | accuracy    0.930\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 146.60s | valid accuracy    0.931 \n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |   500/ 6680 batches | accuracy    0.933\n",
            "| epoch   2 |  1000/ 6680 batches | accuracy    0.932\n",
            "| epoch   2 |  1500/ 6680 batches | accuracy    0.931\n",
            "| epoch   2 |  2000/ 6680 batches | accuracy    0.932\n",
            "| epoch   2 |  2500/ 6680 batches | accuracy    0.930\n",
            "| epoch   2 |  3000/ 6680 batches | accuracy    0.930\n",
            "| epoch   2 |  3500/ 6680 batches | accuracy    0.930\n",
            "| epoch   2 |  4000/ 6680 batches | accuracy    0.933\n",
            "| epoch   2 |  4500/ 6680 batches | accuracy    0.932\n",
            "| epoch   2 |  5000/ 6680 batches | accuracy    0.932\n",
            "| epoch   2 |  5500/ 6680 batches | accuracy    0.934\n",
            "| epoch   2 |  6000/ 6680 batches | accuracy    0.930\n",
            "| epoch   2 |  6500/ 6680 batches | accuracy    0.934\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 145.69s | valid accuracy    0.932 \n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |   500/ 6680 batches | accuracy    0.936\n",
            "| epoch   3 |  1000/ 6680 batches | accuracy    0.932\n",
            "| epoch   3 |  1500/ 6680 batches | accuracy    0.932\n",
            "| epoch   3 |  2000/ 6680 batches | accuracy    0.936\n",
            "| epoch   3 |  2500/ 6680 batches | accuracy    0.932\n",
            "| epoch   3 |  3000/ 6680 batches | accuracy    0.933\n",
            "| epoch   3 |  3500/ 6680 batches | accuracy    0.933\n",
            "| epoch   3 |  4000/ 6680 batches | accuracy    0.934\n",
            "| epoch   3 |  4500/ 6680 batches | accuracy    0.934\n",
            "| epoch   3 |  5000/ 6680 batches | accuracy    0.935\n",
            "| epoch   3 |  5500/ 6680 batches | accuracy    0.934\n",
            "| epoch   3 |  6000/ 6680 batches | accuracy    0.935\n",
            "| epoch   3 |  6500/ 6680 batches | accuracy    0.934\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 147.13s | valid accuracy    0.934 \n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |   500/ 6680 batches | accuracy    0.936\n",
            "| epoch   4 |  1000/ 6680 batches | accuracy    0.935\n",
            "| epoch   4 |  1500/ 6680 batches | accuracy    0.935\n",
            "| epoch   4 |  2000/ 6680 batches | accuracy    0.936\n",
            "| epoch   4 |  2500/ 6680 batches | accuracy    0.937\n",
            "| epoch   4 |  3000/ 6680 batches | accuracy    0.936\n",
            "| epoch   4 |  3500/ 6680 batches | accuracy    0.934\n",
            "| epoch   4 |  4000/ 6680 batches | accuracy    0.935\n",
            "| epoch   4 |  4500/ 6680 batches | accuracy    0.936\n",
            "| epoch   4 |  5000/ 6680 batches | accuracy    0.934\n",
            "| epoch   4 |  5500/ 6680 batches | accuracy    0.936\n",
            "| epoch   4 |  6000/ 6680 batches | accuracy    0.936\n",
            "| epoch   4 |  6500/ 6680 batches | accuracy    0.935\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 148.84s | valid accuracy    0.934 \n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |   500/ 6680 batches | accuracy    0.941\n",
            "| epoch   5 |  1000/ 6680 batches | accuracy    0.939\n",
            "| epoch   5 |  1500/ 6680 batches | accuracy    0.939\n",
            "| epoch   5 |  2000/ 6680 batches | accuracy    0.941\n",
            "| epoch   5 |  2500/ 6680 batches | accuracy    0.941\n",
            "| epoch   5 |  3000/ 6680 batches | accuracy    0.938\n",
            "| epoch   5 |  3500/ 6680 batches | accuracy    0.940\n",
            "| epoch   5 |  4000/ 6680 batches | accuracy    0.940\n",
            "| epoch   5 |  4500/ 6680 batches | accuracy    0.940\n",
            "| epoch   5 |  5000/ 6680 batches | accuracy    0.941\n",
            "| epoch   5 |  5500/ 6680 batches | accuracy    0.941\n",
            "| epoch   5 |  6000/ 6680 batches | accuracy    0.940\n",
            "| epoch   5 |  6500/ 6680 batches | accuracy    0.940\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 145.18s | valid accuracy    0.936 \n",
            "-----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISe3Op1n6rsg"
      },
      "source": [
        "### Objective\n",
        "Pick any 2 datasets (except AG_NEWS) from torchtext.datasets and train your model on them achieving 50% more accuracy than random prediction. Upload to Github with a proper readme file describing your datasets, and showing your logs as well.\n",
        "\n",
        "### Result:\n",
        "\n",
        "SogouNews data set was input data set for classification and this was 5 class classification problem. <br>\n",
        "Objective was to achieving 50% more accuracy than random prediction(20%) which was 30%. <br>\n",
        "Highest validation accuracy acheived was 93.6% which was 50% more accuracy than random prediction. <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzQSvXUMgGkj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}