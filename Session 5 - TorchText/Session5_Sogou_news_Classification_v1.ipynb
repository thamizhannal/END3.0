{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3KbVunpZVFY",
        "outputId": "27c6966d-06ca-4078-b791-dad1f18412af"
      },
      "source": [
        "from torchtext.datasets import SogouNews\n",
        "train_iter = SogouNews(split='train')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 384M/384M [00:02<00:00, 186MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4,\n",
              " '2008 di4 qi1 jie4 qi1ng da3o guo2 ji4 che1 zha3n me3i nv3 mo2 te4  2008di4 qi1 jie4 qi1ng da3o guo2 ji4 che1 zha3n yu2 15 ri4 za4i qi1ng da3o guo2 ji4 hui4 zha3n zho1ng xi1n she4ng da4 ka1i mu4 . be3n ci4 che1 zha3n jia1ng chi2 xu4 da4o be3n yue4 19 ri4 . ji1n nia2n qi1ng da3o guo2 ji4 che1 zha3n shi4 li4 nia2n da3o che2ng che1 zha3n gui1 mo2 zui4 da4 di2 yi1 ci4 , shi3 yo4ng lia3o qi1ng da3o guo2 ji4 hui4 zha3n zho1ng xi1n di2 qua2n bu4 shi4 ne4i wa4i zha3n gua3n . yi3 xia4 we2i xia4n cha3ng mo2 te4 tu2 pia4n .')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbvuK08T8QVA"
      },
      "source": [
        "# list a sample data.\n",
        "next(train_iter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV87TZctgvXU"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_iter = SogouNews(split = 'train')\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fPL7nRXgmcQ"
      },
      "source": [
        "dataloader = DataLoader(train_iter, batch_size=64, shuffle=False)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSdi9eJ7YZN0"
      },
      "source": [
        "# initialize tokenizer\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11XsM-X2Y6-X"
      },
      "source": [
        "#tokenizer = get_tokenizer(\"basic_english\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reQIgCaBfhqK"
      },
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "#train_iter = SogouNews(split = 'train')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "  for _, text in data_iter:\n",
        "    yield tokenizer(text)\n",
        "\n",
        "# build vocab object.\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siUDwNH0fpv6"
      },
      "source": [
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) - 1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49fMg0LofrnK"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list.to(device), text_list.to(device), offsets.to(device) "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIj1zjq5fvJC"
      },
      "source": [
        "train_iter = SogouNews(split='train')\n",
        "dataloader = DataLoader(train_iter, batch_size=64, shuffle=True, collate_fn=collate_batch)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebIF9o9Sf6Ob"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxp5t0QJf86g"
      },
      "source": [
        "train_iter = SogouNews(split='train')\n",
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "vocab_size = len(vocab)\n",
        "emsize = 64\n",
        "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S70OsJRjgC4g"
      },
      "source": [
        "import time\n",
        "\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        predited_label = model(text, offsets)\n",
        "        loss = criterion(predited_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1) # disuccees\n",
        "        optimizer.step()\n",
        "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "            predited_label = model(text, offsets)\n",
        "            loss = criterion(predited_label, label)\n",
        "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBD86_SxgDW2",
        "outputId": "6ae8d6f8-e553-41f9-9037-e70bb6e7d8e3"
      },
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "# Hyperparameters\n",
        "EPOCHS = 5 # epoch\n",
        "LR = 5  # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training\n",
        "  \n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "total_accu = None\n",
        "train_iter, test_iter = SogouNews()\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   500/ 6680 batches | accuracy    0.814\n",
            "| epoch   1 |  1000/ 6680 batches | accuracy    0.908\n",
            "| epoch   1 |  1500/ 6680 batches | accuracy    0.916\n",
            "| epoch   1 |  2000/ 6680 batches | accuracy    0.921\n",
            "| epoch   1 |  2500/ 6680 batches | accuracy    0.924\n",
            "| epoch   1 |  3000/ 6680 batches | accuracy    0.923\n",
            "| epoch   1 |  3500/ 6680 batches | accuracy    0.926\n",
            "| epoch   1 |  4000/ 6680 batches | accuracy    0.925\n",
            "| epoch   1 |  4500/ 6680 batches | accuracy    0.927\n",
            "| epoch   1 |  5000/ 6680 batches | accuracy    0.928\n",
            "| epoch   1 |  5500/ 6680 batches | accuracy    0.927\n",
            "| epoch   1 |  6000/ 6680 batches | accuracy    0.927\n",
            "| epoch   1 |  6500/ 6680 batches | accuracy    0.930\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 133.01s | valid accuracy    0.929 \n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |   500/ 6680 batches | accuracy    0.929\n",
            "| epoch   2 |  1000/ 6680 batches | accuracy    0.932\n",
            "| epoch   2 |  1500/ 6680 batches | accuracy    0.931\n",
            "| epoch   2 |  2000/ 6680 batches | accuracy    0.931\n",
            "| epoch   2 |  2500/ 6680 batches | accuracy    0.930\n",
            "| epoch   2 |  3000/ 6680 batches | accuracy    0.932\n",
            "| epoch   2 |  3500/ 6680 batches | accuracy    0.931\n",
            "| epoch   2 |  4000/ 6680 batches | accuracy    0.932\n",
            "| epoch   2 |  4500/ 6680 batches | accuracy    0.931\n",
            "| epoch   2 |  5000/ 6680 batches | accuracy    0.932\n",
            "| epoch   2 |  5500/ 6680 batches | accuracy    0.931\n",
            "| epoch   2 |  6000/ 6680 batches | accuracy    0.935\n",
            "| epoch   2 |  6500/ 6680 batches | accuracy    0.933\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 132.00s | valid accuracy    0.934 \n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |   500/ 6680 batches | accuracy    0.931\n",
            "| epoch   3 |  1000/ 6680 batches | accuracy    0.936\n",
            "| epoch   3 |  1500/ 6680 batches | accuracy    0.935\n",
            "| epoch   3 |  2000/ 6680 batches | accuracy    0.935\n",
            "| epoch   3 |  2500/ 6680 batches | accuracy    0.934\n",
            "| epoch   3 |  3000/ 6680 batches | accuracy    0.934\n",
            "| epoch   3 |  3500/ 6680 batches | accuracy    0.934\n",
            "| epoch   3 |  4000/ 6680 batches | accuracy    0.935\n",
            "| epoch   3 |  4500/ 6680 batches | accuracy    0.934\n",
            "| epoch   3 |  5000/ 6680 batches | accuracy    0.932\n",
            "| epoch   3 |  5500/ 6680 batches | accuracy    0.934\n",
            "| epoch   3 |  6000/ 6680 batches | accuracy    0.933\n",
            "| epoch   3 |  6500/ 6680 batches | accuracy    0.932\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 132.39s | valid accuracy    0.926 \n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |   500/ 6680 batches | accuracy    0.938\n",
            "| epoch   4 |  1000/ 6680 batches | accuracy    0.937\n",
            "| epoch   4 |  1500/ 6680 batches | accuracy    0.939\n",
            "| epoch   4 |  2000/ 6680 batches | accuracy    0.938\n",
            "| epoch   4 |  2500/ 6680 batches | accuracy    0.939\n",
            "| epoch   4 |  3000/ 6680 batches | accuracy    0.939\n",
            "| epoch   4 |  3500/ 6680 batches | accuracy    0.937\n",
            "| epoch   4 |  4000/ 6680 batches | accuracy    0.938\n",
            "| epoch   4 |  4500/ 6680 batches | accuracy    0.940\n",
            "| epoch   4 |  5000/ 6680 batches | accuracy    0.940\n",
            "| epoch   4 |  5500/ 6680 batches | accuracy    0.939\n",
            "| epoch   4 |  6000/ 6680 batches | accuracy    0.940\n",
            "| epoch   4 |  6500/ 6680 batches | accuracy    0.940\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 132.55s | valid accuracy    0.938 \n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |   500/ 6680 batches | accuracy    0.939\n",
            "| epoch   5 |  1000/ 6680 batches | accuracy    0.940\n",
            "| epoch   5 |  1500/ 6680 batches | accuracy    0.939\n",
            "| epoch   5 |  2000/ 6680 batches | accuracy    0.939\n",
            "| epoch   5 |  2500/ 6680 batches | accuracy    0.940\n",
            "| epoch   5 |  3000/ 6680 batches | accuracy    0.939\n",
            "| epoch   5 |  3500/ 6680 batches | accuracy    0.941\n",
            "| epoch   5 |  4000/ 6680 batches | accuracy    0.937\n",
            "| epoch   5 |  4500/ 6680 batches | accuracy    0.941\n",
            "| epoch   5 |  5000/ 6680 batches | accuracy    0.938\n",
            "| epoch   5 |  5500/ 6680 batches | accuracy    0.943\n",
            "| epoch   5 |  6000/ 6680 batches | accuracy    0.940\n",
            "| epoch   5 |  6500/ 6680 batches | accuracy    0.938\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 131.58s | valid accuracy    0.937 \n",
            "-----------------------------------------------------------\n",
            "| epoch   6 |   500/ 6680 batches | accuracy    0.941\n",
            "| epoch   6 |  1000/ 6680 batches | accuracy    0.941\n",
            "| epoch   6 |  1500/ 6680 batches | accuracy    0.943\n",
            "| epoch   6 |  2000/ 6680 batches | accuracy    0.938\n",
            "| epoch   6 |  2500/ 6680 batches | accuracy    0.939\n",
            "| epoch   6 |  3000/ 6680 batches | accuracy    0.939\n",
            "| epoch   6 |  3500/ 6680 batches | accuracy    0.942\n",
            "| epoch   6 |  4000/ 6680 batches | accuracy    0.938\n",
            "| epoch   6 |  4500/ 6680 batches | accuracy    0.938\n",
            "| epoch   6 |  5000/ 6680 batches | accuracy    0.941\n",
            "| epoch   6 |  5500/ 6680 batches | accuracy    0.940\n",
            "| epoch   6 |  6000/ 6680 batches | accuracy    0.942\n",
            "| epoch   6 |  6500/ 6680 batches | accuracy    0.939\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time: 132.19s | valid accuracy    0.938 \n",
            "-----------------------------------------------------------\n",
            "| epoch   7 |   500/ 6680 batches | accuracy    0.940\n",
            "| epoch   7 |  1000/ 6680 batches | accuracy    0.942\n",
            "| epoch   7 |  1500/ 6680 batches | accuracy    0.940\n",
            "| epoch   7 |  2000/ 6680 batches | accuracy    0.940\n",
            "| epoch   7 |  2500/ 6680 batches | accuracy    0.939\n",
            "| epoch   7 |  3000/ 6680 batches | accuracy    0.940\n",
            "| epoch   7 |  3500/ 6680 batches | accuracy    0.938\n",
            "| epoch   7 |  4000/ 6680 batches | accuracy    0.943\n",
            "| epoch   7 |  4500/ 6680 batches | accuracy    0.941\n",
            "| epoch   7 |  5000/ 6680 batches | accuracy    0.938\n",
            "| epoch   7 |  5500/ 6680 batches | accuracy    0.939\n",
            "| epoch   7 |  6000/ 6680 batches | accuracy    0.939\n",
            "| epoch   7 |  6500/ 6680 batches | accuracy    0.942\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time: 133.23s | valid accuracy    0.938 \n",
            "-----------------------------------------------------------\n",
            "| epoch   8 |   500/ 6680 batches | accuracy    0.941\n",
            "| epoch   8 |  1000/ 6680 batches | accuracy    0.940\n",
            "| epoch   8 |  1500/ 6680 batches | accuracy    0.939\n",
            "| epoch   8 |  2000/ 6680 batches | accuracy    0.942\n",
            "| epoch   8 |  2500/ 6680 batches | accuracy    0.940\n",
            "| epoch   8 |  3000/ 6680 batches | accuracy    0.939\n",
            "| epoch   8 |  3500/ 6680 batches | accuracy    0.942\n",
            "| epoch   8 |  4000/ 6680 batches | accuracy    0.940\n",
            "| epoch   8 |  4500/ 6680 batches | accuracy    0.938\n",
            "| epoch   8 |  5000/ 6680 batches | accuracy    0.940\n",
            "| epoch   8 |  5500/ 6680 batches | accuracy    0.943\n",
            "| epoch   8 |  6000/ 6680 batches | accuracy    0.942\n",
            "| epoch   8 |  6500/ 6680 batches | accuracy    0.938\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time: 135.74s | valid accuracy    0.938 \n",
            "-----------------------------------------------------------\n",
            "| epoch   9 |   500/ 6680 batches | accuracy    0.938\n",
            "| epoch   9 |  1000/ 6680 batches | accuracy    0.941\n",
            "| epoch   9 |  1500/ 6680 batches | accuracy    0.941\n",
            "| epoch   9 |  2000/ 6680 batches | accuracy    0.941\n",
            "| epoch   9 |  2500/ 6680 batches | accuracy    0.940\n",
            "| epoch   9 |  3000/ 6680 batches | accuracy    0.941\n",
            "| epoch   9 |  3500/ 6680 batches | accuracy    0.941\n",
            "| epoch   9 |  4000/ 6680 batches | accuracy    0.940\n",
            "| epoch   9 |  4500/ 6680 batches | accuracy    0.941\n",
            "| epoch   9 |  5000/ 6680 batches | accuracy    0.940\n",
            "| epoch   9 |  5500/ 6680 batches | accuracy    0.939\n",
            "| epoch   9 |  6000/ 6680 batches | accuracy    0.941\n",
            "| epoch   9 |  6500/ 6680 batches | accuracy    0.939\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time: 131.20s | valid accuracy    0.938 \n",
            "-----------------------------------------------------------\n",
            "| epoch  10 |   500/ 6680 batches | accuracy    0.940\n",
            "| epoch  10 |  1000/ 6680 batches | accuracy    0.942\n",
            "| epoch  10 |  1500/ 6680 batches | accuracy    0.941\n",
            "| epoch  10 |  2000/ 6680 batches | accuracy    0.941\n",
            "| epoch  10 |  2500/ 6680 batches | accuracy    0.939\n",
            "| epoch  10 |  3000/ 6680 batches | accuracy    0.940\n",
            "| epoch  10 |  3500/ 6680 batches | accuracy    0.940\n",
            "| epoch  10 |  4000/ 6680 batches | accuracy    0.942\n",
            "| epoch  10 |  4500/ 6680 batches | accuracy    0.940\n",
            "| epoch  10 |  5000/ 6680 batches | accuracy    0.940\n",
            "| epoch  10 |  5500/ 6680 batches | accuracy    0.939\n",
            "| epoch  10 |  6000/ 6680 batches | accuracy    0.939\n",
            "| epoch  10 |  6500/ 6680 batches | accuracy    0.941\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time: 142.00s | valid accuracy    0.938 \n",
            "-----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzQSvXUMgGkj"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}