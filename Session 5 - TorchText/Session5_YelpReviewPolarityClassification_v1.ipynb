{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qcNBjj7T0_B"
      },
      "source": [
        "# load pytorch library.\n",
        "\n",
        "from torchtext.datasets import YelpReviewPolarity\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qav5EUWXynRn"
      },
      "source": [
        "### Yelp Review Polarity Data Description\n",
        "The Yelp reviews polarity dataset is constructed by considering stars 1 and 2 negative, and 3 and 4 positive. For each polarity 280,000 training samples and 19,000 testing samples are take randomly. In total there are 560,000 trainig samples and 38,000 testing samples. Negative polarity is class 1, and positive class 2.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKk7vA9hZFlh"
      },
      "source": [
        "# Load Yelp Reivew Polarity dataset.\n",
        "train_iter, test_iter = YelpReviewPolarity(split=('train','test'))\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T1--f5Sco1v",
        "outputId": "ea46c48e-c1a7-4a60-d63f-d58590c0613b"
      },
      "source": [
        "# list a sample data.\n",
        "next(train_iter)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,\n",
              " \"I don't know what Dr. Goldberg was like before  moving to Arizona, but let me tell you, STAY AWAY from this doctor and this office. I was going to Dr. Johnson before he left and Goldberg took over when Johnson left. He is not a caring doctor. He is only interested in the co-pay and having you come in for medication refills every month. He will not give refills and could less about patients's financial situations. Trying to get your 90 days mail away pharmacy prescriptions through this guy is a joke. And to make matters even worse, his office staff is incompetent. 90% of the time when you call the office, they'll put you through to a voice mail, that NO ONE ever answers or returns your call. Both my adult children and husband have decided to leave this practice after experiencing such frustration. The entire office has an attitude like they are doing you a favor. Give me a break! Stay away from this doc and the practice. You deserve better and they will not be there when you really need them. I have never felt compelled to write a bad review about anyone until I met this pathetic excuse for a doctor who is all about the money.\")"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcC-A8N1yF-h",
        "outputId": "dd9fe9af-82f0-4cf7-976c-6792e926d6cb"
      },
      "source": [
        "next(test_iter)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2,\n",
              " \"Contrary to other reviews, I have zero complaints about the service or the prices. I have been getting tire service here for the past 5 years now, and compared to my experience with places like Pep Boys, these guys are experienced and know what they're doing. \\\\nAlso, this is one place that I do not feel like I am being taken advantage of, just because of my gender. Other auto mechanics have been notorious for capitalizing on my ignorance of cars, and have sucked my bank account dry. But here, my service and road coverage has all been well explained - and let up to me to decide. \\\\nAnd they just renovated the waiting room. It looks a lot better than it did in previous years.\")"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlcKBWYxcN4Q"
      },
      "source": [
        "dataloader = DataLoader(train_iter, batch_size=64, shuffle=False)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7Jje0fbZOfz"
      },
      "source": [
        "# initialize tokenizer\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3KbVunpZVFY"
      },
      "source": [
        "# Initialize english tokernizer\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "  for _, text in data_iter:\n",
        "    yield tokenizer(text)\n",
        "\n",
        "# build vocab object.\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV87TZctgvXU"
      },
      "source": [
        "# Build text and label pipeline\n",
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) - 1"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fPL7nRXgmcQ"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
        "    "
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSdi9eJ7YZN0"
      },
      "source": [
        "train_iter = YelpReviewPolarity(split='train')\n",
        "# data loader using collate_batch fn\n",
        "dataloader = DataLoader(train_iter, batch_size=64, shuffle=False, collate_fn=collate_batch)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11XsM-X2Y6-X"
      },
      "source": [
        "from torch import nn\n",
        "# Text Classification Network\n",
        "class TextClassificationModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reQIgCaBfhqK"
      },
      "source": [
        "# Define Network parameters\n",
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "vocab_size = len(vocab)\n",
        "emsize = 64\n",
        "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siUDwNH0fpv6"
      },
      "source": [
        "import time\n",
        "# Model train\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        predited_label = model(text, offsets)\n",
        "        loss = criterion(predited_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1) \n",
        "        optimizer.step()\n",
        "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "# Model validation\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "            predited_label = model(text, offsets)\n",
        "            loss = criterion(predited_label, label)\n",
        "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49fMg0LofrnK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd5ff580-70d4-4977-fcc3-0367706fd799"
      },
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "# Hyperparameters\n",
        "EPOCHS = 5 # epoch\n",
        "LR = 5  # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training\n",
        "  \n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "total_accu = None\n",
        "train_iter, test_iter = YelpReviewPolarity()\n",
        "\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   500/ 8313 batches | accuracy    0.779\n",
            "| epoch   1 |  1000/ 8313 batches | accuracy    0.863\n",
            "| epoch   1 |  1500/ 8313 batches | accuracy    0.880\n",
            "| epoch   1 |  2000/ 8313 batches | accuracy    0.889\n",
            "| epoch   1 |  2500/ 8313 batches | accuracy    0.896\n",
            "| epoch   1 |  3000/ 8313 batches | accuracy    0.898\n",
            "| epoch   1 |  3500/ 8313 batches | accuracy    0.903\n",
            "| epoch   1 |  4000/ 8313 batches | accuracy    0.900\n",
            "| epoch   1 |  4500/ 8313 batches | accuracy    0.904\n",
            "| epoch   1 |  5000/ 8313 batches | accuracy    0.905\n",
            "| epoch   1 |  5500/ 8313 batches | accuracy    0.907\n",
            "| epoch   1 |  6000/ 8313 batches | accuracy    0.911\n",
            "| epoch   1 |  6500/ 8313 batches | accuracy    0.908\n",
            "| epoch   1 |  7000/ 8313 batches | accuracy    0.907\n",
            "| epoch   1 |  7500/ 8313 batches | accuracy    0.911\n",
            "| epoch   1 |  8000/ 8313 batches | accuracy    0.911\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 61.64s | valid accuracy    0.912 \n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |   500/ 8313 batches | accuracy    0.918\n",
            "| epoch   2 |  1000/ 8313 batches | accuracy    0.915\n",
            "| epoch   2 |  1500/ 8313 batches | accuracy    0.917\n",
            "| epoch   2 |  2000/ 8313 batches | accuracy    0.916\n",
            "| epoch   2 |  2500/ 8313 batches | accuracy    0.915\n",
            "| epoch   2 |  3000/ 8313 batches | accuracy    0.917\n",
            "| epoch   2 |  3500/ 8313 batches | accuracy    0.918\n",
            "| epoch   2 |  4000/ 8313 batches | accuracy    0.916\n",
            "| epoch   2 |  4500/ 8313 batches | accuracy    0.918\n",
            "| epoch   2 |  5000/ 8313 batches | accuracy    0.918\n",
            "| epoch   2 |  5500/ 8313 batches | accuracy    0.916\n",
            "| epoch   2 |  6000/ 8313 batches | accuracy    0.920\n",
            "| epoch   2 |  6500/ 8313 batches | accuracy    0.919\n",
            "| epoch   2 |  7000/ 8313 batches | accuracy    0.922\n",
            "| epoch   2 |  7500/ 8313 batches | accuracy    0.921\n",
            "| epoch   2 |  8000/ 8313 batches | accuracy    0.918\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 61.66s | valid accuracy    0.913 \n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |   500/ 8313 batches | accuracy    0.925\n",
            "| epoch   3 |  1000/ 8313 batches | accuracy    0.922\n",
            "| epoch   3 |  1500/ 8313 batches | accuracy    0.920\n",
            "| epoch   3 |  2000/ 8313 batches | accuracy    0.921\n",
            "| epoch   3 |  2500/ 8313 batches | accuracy    0.922\n",
            "| epoch   3 |  3000/ 8313 batches | accuracy    0.922\n",
            "| epoch   3 |  3500/ 8313 batches | accuracy    0.921\n",
            "| epoch   3 |  4000/ 8313 batches | accuracy    0.923\n",
            "| epoch   3 |  4500/ 8313 batches | accuracy    0.923\n",
            "| epoch   3 |  5000/ 8313 batches | accuracy    0.923\n",
            "| epoch   3 |  5500/ 8313 batches | accuracy    0.922\n",
            "| epoch   3 |  6000/ 8313 batches | accuracy    0.922\n",
            "| epoch   3 |  6500/ 8313 batches | accuracy    0.923\n",
            "| epoch   3 |  7000/ 8313 batches | accuracy    0.924\n",
            "| epoch   3 |  7500/ 8313 batches | accuracy    0.925\n",
            "| epoch   3 |  8000/ 8313 batches | accuracy    0.924\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 61.70s | valid accuracy    0.914 \n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |   500/ 8313 batches | accuracy    0.925\n",
            "| epoch   4 |  1000/ 8313 batches | accuracy    0.925\n",
            "| epoch   4 |  1500/ 8313 batches | accuracy    0.926\n",
            "| epoch   4 |  2000/ 8313 batches | accuracy    0.926\n",
            "| epoch   4 |  2500/ 8313 batches | accuracy    0.928\n",
            "| epoch   4 |  3000/ 8313 batches | accuracy    0.922\n",
            "| epoch   4 |  3500/ 8313 batches | accuracy    0.926\n",
            "| epoch   4 |  4000/ 8313 batches | accuracy    0.926\n",
            "| epoch   4 |  4500/ 8313 batches | accuracy    0.928\n",
            "| epoch   4 |  5000/ 8313 batches | accuracy    0.925\n",
            "| epoch   4 |  5500/ 8313 batches | accuracy    0.925\n",
            "| epoch   4 |  6000/ 8313 batches | accuracy    0.923\n",
            "| epoch   4 |  6500/ 8313 batches | accuracy    0.929\n",
            "| epoch   4 |  7000/ 8313 batches | accuracy    0.927\n",
            "| epoch   4 |  7500/ 8313 batches | accuracy    0.927\n",
            "| epoch   4 |  8000/ 8313 batches | accuracy    0.926\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 62.15s | valid accuracy    0.927 \n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |   500/ 8313 batches | accuracy    0.928\n",
            "| epoch   5 |  1000/ 8313 batches | accuracy    0.928\n",
            "| epoch   5 |  1500/ 8313 batches | accuracy    0.929\n",
            "| epoch   5 |  2000/ 8313 batches | accuracy    0.926\n",
            "| epoch   5 |  2500/ 8313 batches | accuracy    0.931\n",
            "| epoch   5 |  3000/ 8313 batches | accuracy    0.929\n",
            "| epoch   5 |  3500/ 8313 batches | accuracy    0.930\n",
            "| epoch   5 |  4000/ 8313 batches | accuracy    0.928\n",
            "| epoch   5 |  4500/ 8313 batches | accuracy    0.929\n",
            "| epoch   5 |  5000/ 8313 batches | accuracy    0.927\n",
            "| epoch   5 |  5500/ 8313 batches | accuracy    0.927\n",
            "| epoch   5 |  6000/ 8313 batches | accuracy    0.929\n",
            "| epoch   5 |  6500/ 8313 batches | accuracy    0.927\n",
            "| epoch   5 |  7000/ 8313 batches | accuracy    0.930\n",
            "| epoch   5 |  7500/ 8313 batches | accuracy    0.926\n",
            "| epoch   5 |  8000/ 8313 batches | accuracy    0.930\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 61.61s | valid accuracy    0.929 \n",
            "-----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocnE_juUue4o"
      },
      "source": [
        "### Result:\n",
        "Pick any 2 datasets (except AG_NEWS) from torchtext.datasets and train your model on them achieving 50% more accuracy than random prediction. \n",
        "Upload to Github with a proper readme file describing your datasets, and showing your logs as well. \n",
        "\n",
        "1. Yelp Review Plarity data set was input data set for classification and this was two class classification problem.\n",
        "2. Objective was to chieving 50% more accuracy(75%) than random prediction(50%) which was 75%\n",
        "3. Highest validation accuracy acheived was 92.9% which was 50% more accuracy than random prediction\n",
        "\n"
      ]
    }
  ]
}